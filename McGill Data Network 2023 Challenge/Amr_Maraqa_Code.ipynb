{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e5f4bb",
   "metadata": {},
   "source": [
    "# MDN Data Challenge\n",
    "### Name: *Amr Maraqa* ID: *261101609*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e22c031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from numpy import where \n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d12d07",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ed03e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\m.maraqa\\\\Desktop\\\\MMA courses\\\\MDN Data Challenge\\\\Past_Students.csv')\n",
    "test = pd.read_csv('C:\\\\Users\\\\m.maraqa\\\\Desktop\\\\MMA courses\\\\MDN Data Challenge\\\\Graduating_Class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1a8d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial_No</th>\n",
       "      <th>GRE_Score</th>\n",
       "      <th>TOEFL_Score</th>\n",
       "      <th>University_Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance_of_Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>346</td>\n",
       "      <td>316</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>347</td>\n",
       "      <td>304</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>348</td>\n",
       "      <td>299</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>349</td>\n",
       "      <td>302</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>350</td>\n",
       "      <td>313</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial_No  GRE_Score  TOEFL_Score  University_Rating  SOP  LOR  CGPA  \\\n",
       "0            1        337          118                  4  4.5  4.5  9.65   \n",
       "1            2        324          107                  4  4.0  4.5  8.87   \n",
       "2            3        316          104                  3  3.0  3.5  8.00   \n",
       "3            4        322          110                  3  3.5  2.5  8.67   \n",
       "4            5        314          103                  2  2.0  3.0  8.21   \n",
       "..         ...        ...          ...                ...  ...  ...   ...   \n",
       "345        346        316           98                  1  1.5  2.0  7.43   \n",
       "346        347        304           97                  2  1.5  2.0  7.64   \n",
       "347        348        299           94                  1  1.0  1.0  7.34   \n",
       "348        349        302           99                  1  2.0  2.0  7.25   \n",
       "349        350        313          101                  3  2.5  3.0  8.04   \n",
       "\n",
       "     Research  Chance_of_Admit  \n",
       "0           1             0.92  \n",
       "1           1             0.76  \n",
       "2           1             0.72  \n",
       "3           1             0.80  \n",
       "4           0             0.65  \n",
       "..        ...              ...  \n",
       "345         0             0.49  \n",
       "346         0             0.47  \n",
       "347         0             0.42  \n",
       "348         0             0.57  \n",
       "349         0             0.62  \n",
       "\n",
       "[350 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cf9d3",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0a5aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial_No            False\n",
       "GRE_Score            False\n",
       "TOEFL_Score          False\n",
       "University_Rating    False\n",
       "SOP                  False\n",
       "LOR                  False\n",
       "CGPA                 False\n",
       "Research             False\n",
       "Chance_of_Admit      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NA values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd64b2",
   "metadata": {},
   "source": [
    "No missing values found. No imputation required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b0bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping identifier variables unnecessary for prediction\n",
    "df = df.drop([df.columns[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901a6a4",
   "metadata": {},
   "source": [
    "## Detect and remove anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b67ae2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_Score</th>\n",
       "      <th>TOEFL_Score</th>\n",
       "      <th>University_Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance_of_Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>295</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>316</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>304</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>302</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>313</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE_Score  TOEFL_Score  University_Rating  SOP  LOR  CGPA  Research  \\\n",
       "0          337          118                  4  4.5  4.5  9.65         1   \n",
       "1          324          107                  4  4.0  4.5  8.87         1   \n",
       "2          316          104                  3  3.0  3.5  8.00         1   \n",
       "3          322          110                  3  3.5  2.5  8.67         1   \n",
       "4          314          103                  2  2.0  3.0  8.21         0   \n",
       "..         ...          ...                ...  ...  ...   ...       ...   \n",
       "344        295           96                  2  1.5  2.0  7.34         0   \n",
       "345        316           98                  1  1.5  2.0  7.43         0   \n",
       "346        304           97                  2  1.5  2.0  7.64         0   \n",
       "348        302           99                  1  2.0  2.0  7.25         0   \n",
       "349        313          101                  3  2.5  3.0  8.04         0   \n",
       "\n",
       "     Chance_of_Admit  \n",
       "0               0.92  \n",
       "1               0.76  \n",
       "2               0.72  \n",
       "3               0.80  \n",
       "4               0.65  \n",
       "..               ...  \n",
       "344             0.47  \n",
       "345             0.49  \n",
       "346             0.47  \n",
       "348             0.57  \n",
       "349             0.62  \n",
       "\n",
       "[343 rows x 8 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9c0be0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1]\n",
      "(array([ 27,  28,  76, 114, 197, 198, 267], dtype=int64),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22360\\3455130185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0manom_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manom_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnon_anom_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m         \u001b[0mtup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;31m# check that the key does not exceed the maximum size of the index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Can only index by location with a [{self._valid_types}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Detecting anomalies\n",
    "iforest=IsolationForest(n_estimators=100,contamination=0.02,random_state=5)\n",
    "\n",
    "pred=iforest.fit_predict(df)\n",
    "print(pred)\n",
    "\n",
    "# Dropping anomalies\n",
    "non_anom_index=where(pred==1)\n",
    "df = df.iloc[non_anom_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f82782de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  59,  60,  61,  62,  63,  64,  65,  66,\n",
       "         67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "         80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  92,  93,\n",
       "         94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
       "        107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
       "        120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
       "        133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146,\n",
       "        147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,\n",
       "        160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173,\n",
       "        174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186,\n",
       "        187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
       "        200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
       "        213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
       "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
       "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
       "        265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 277, 278,\n",
       "        279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291,\n",
       "        292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304,\n",
       "        305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317,\n",
       "        318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330,\n",
       "        331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
       "        344, 345, 346, 348, 349], dtype=int64),)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_anom_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d8ad3",
   "metadata": {},
   "source": [
    "## Train + Validation Set Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df31d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining target and predictors\n",
    "X = df.drop('Chance_of_Admit', axis = 1)\n",
    "y = df['Chance_of_Admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5912422d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    343.000000\n",
       "mean       3.481050\n",
       "std        0.891196\n",
       "min        1.500000\n",
       "25%        3.000000\n",
       "50%        3.500000\n",
       "75%        4.000000\n",
       "max        5.000000\n",
       "Name: LOR, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['LOR'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe11d576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz7ElEQVR4nO3de1xU9b7/8fcCcQQDDU2IQqVE867lJbULZuBxl2Hsjrm11DKzo5a38nJMHW+YejJ2urPcu7zUZttjZ1q5TcEumJonUak0u7vRTDa7JECxcYR1/vDn/JoQY2SGGde8no/HPHR915r1/axZA7wf3/WdNYZpmqYAAAAsKsTfBQAAAPgSYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFhaHX8XEAgqKir0/fffKzIyUoZh+LscAABQDaZpqrS0VHFxcQoJqXr8hrAj6fvvv1d8fLy/ywAAABfhyJEjuvrqq6tcT9iRFBkZKensixUVFeXnamqP0+lUVlaWUlJSFBYW5u9y4GOc7+DC+Q4uwXq+S0pKFB8f7/o7XhXCjuS6dBUVFRV0YSciIkJRUVFB9cMRrDjfwYXzHVyC/Xz/1hQUJigDAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLI+wAAABLq+PvAgDAyppP/Ydf+rWFmlrUTWpn3yJHuVHr/f/zqTtqvU+gKozsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS/Nr2Nm2bZv69++vuLg4GYahDRs2uNY5nU5NmTJF7du3V/369RUXF6ehQ4fq+++/d9uHw+HQo48+qsaNG6t+/fq666679N1339XykQAAgEDl17Bz8uRJdezYUcuWLau0rqysTHv37tWMGTO0d+9evf766/ryyy911113uW03fvx4rV+/XmvXrtX27dt14sQJ3XnnnSovL6+twwAAAAHMrzcV7Nevn/r163fedQ0aNFB2drZb29KlS9WtWzcdPnxYTZs2VXFxsV588UW9/PLLuv322yVJr7zyiuLj47V161b17dvX58cAAAAC2yV1B+Xi4mIZhqGGDRtKkvbs2SOn06mUlBTXNnFxcWrXrp127txZZdhxOBxyOByu5ZKSEklnL505nU7fHUCAOXeswXTMwYzz7R+2UNM//YaYbv/WNt5ntStYf76re7yXTNj5+eefNXXqVA0ePFhRUVGSpIKCAtWtW1eXX36527YxMTEqKCiocl8LFizQ7NmzK7VnZWUpIiLCu4VfAn49ggZr43zXrkXd/Nv/3C4Vful306ZNfuk32AXbz3dZWVm1trskwo7T6dSgQYNUUVGh55577je3N01ThlH1d8FMmzZNEydOdC2XlJQoPj5eKSkpriAVDJxOp7Kzs5WcnKywsDB/lwMf43z7Rzv7Fr/0awsxNbdLhWbkhshRUfvfjbXfzjSC2hSsP9/nrsz8loAPO06nUwMHDtShQ4f07rvvuoWR2NhYnT59WkVFRW6jO4WFherZs2eV+7TZbLLZbJXaw8LCgupNck6wHnew4nzXLn98Cadb/xWGX2rgPeYfwfbzXd1jDej77JwLOl999ZW2bt2qRo0aua2/4YYbFBYW5jZsd+zYMe3fv/+CYQcAAAQPv47snDhxQl9//bVr+dChQ8rLy1N0dLTi4uJ0zz33aO/evdq4caPKy8td83Cio6NVt25dNWjQQCNGjNCkSZPUqFEjRUdH6/HHH1f79u1dn84CAADBza9hJzc3V71793Ytn5tHM2zYMNntdr355puSpE6dOrk977333lNSUpIk6ZlnnlGdOnU0cOBAnTp1Sn369NGqVasUGhpaK8cAAAACm1/DTlJSkkyz6o9FXmjdOfXq1dPSpUu1dOlSb5YGAAAsIqDn7AAAANQUYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFhaHX8XAASb5lP/4Zd+baGmFnWT2tm3yFFu1Hr//3zqjlrvEwAkRnYAAIDFEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAIClEXYAAICl+TXsbNu2Tf3791dcXJwMw9CGDRvc1pumKbvdrri4OIWHhyspKUkHDhxw28bhcOjRRx9V48aNVb9+fd1111367rvvavEoAABAIPNr2Dl58qQ6duyoZcuWnXf9okWLtGTJEi1btky7d+9WbGyskpOTVVpa6tpm/PjxWr9+vdauXavt27frxIkTuvPOO1VeXl5bhwEAAAJYHX923q9fP/Xr1++860zTVEZGhqZPn660tDRJ0urVqxUTE6PMzEyNGjVKxcXFevHFF/Xyyy/r9ttvlyS98sorio+P19atW9W3b99aOxYAABCY/Bp2LuTQoUMqKChQSkqKq81ms+nWW2/Vzp07NWrUKO3Zs0dOp9Ntm7i4OLVr1047d+6sMuw4HA45HA7XcklJiSTJ6XTK6XT66IgCz7ljDaZjDgS2UNM//YaYbv/WtmB9n3G+URuC9fd5dY83YMNOQUGBJCkmJsatPSYmRvn5+a5t6tatq8svv7zSNueefz4LFizQ7NmzK7VnZWUpIiKipqVfcrKzs/1dQlBZ1M2//c/tUuGXfjdt2uSXfv2N843aFGy/z8vKyqq1XcCGnXMMw3BbNk2zUtuv/dY206ZN08SJE13LJSUlio+PV0pKiqKiompW8CXE6XQqOztbycnJCgsL83c5QaOdfYtf+rWFmJrbpUIzckPkqLjwz5Av7LcH52VlzjdqQ7D+Pj93Zea3BGzYiY2NlXR29ObKK690tRcWFrpGe2JjY3X69GkVFRW5je4UFhaqZ8+eVe7bZrPJZrNVag8LCwuqN8k5wXrc/uIor/0/PG79Vxh+qSFY32Ocb9SmYPt9Xt1jDdj77CQkJCg2NtZtSO706dPKyclxBZkbbrhBYWFhbtscO3ZM+/fvv2DYAQAAwcOvIzsnTpzQ119/7Vo+dOiQ8vLyFB0draZNm2r8+PFKT09XYmKiEhMTlZ6eroiICA0ePFiS1KBBA40YMUKTJk1So0aNFB0drccff1zt27d3fToLAAAEN7+GndzcXPXu3du1fG4ezbBhw7Rq1SpNnjxZp06d0ujRo1VUVKTu3bsrKytLkZGRruc888wzqlOnjgYOHKhTp06pT58+WrVqlUJDQ2v9eAAAQODxa9hJSkqSaVb9sUjDMGS322W326vcpl69elq6dKmWLl3qgwoBAMClLmDn7AAAAHgDYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFgaYQcAAFiax2Hn1KlTKisrcy3n5+crIyNDWVlZXi0MAADAGzwOO6mpqVqzZo0k6aefflL37t319NNPKzU1VcuXL/d6gQAAADXhcdjZu3evbr75ZknSa6+9ppiYGOXn52vNmjV69tlnvV4gAABATXgcdsrKyhQZGSlJysrKUlpamkJCQnTjjTcqPz/f6wUCAADUhMdhp0WLFtqwYYOOHDmiLVu2KCUlRZJUWFioqKgorxcIAABQEx6HnZkzZ+rxxx9X8+bN1b17d/Xo0UPS2VGezp07e71AAACAmqjj6RPuuece3XTTTTp27Jg6duzoau/Tp4/S0tK8WhwAAEBNeTyy8+CDD6p+/frq3LmzQkL+/9Pbtm2rhQsXerU4AACAmvI47KxevVqnTp2q1H7q1CnXR9IBAAACRbUvY5WUlMg0TZmmqdLSUtWrV8+1rry8XJs2bVKTJk18UiQAAMDFqnbYadiwoQzDkGEYatmyZaX1hmFo9uzZXi0OAACgpqoddt577z2ZpqnbbrtN69atU3R0tGtd3bp11axZM8XFxfmkSAAAgItV7bBz6623SpIOHTqk+Ph4t8nJAAAAgcrjj543a9ZMP/30kz766CMVFhaqoqLCbf3QoUO9VhwAAEBNeRx23nrrLQ0ZMkQnT55UZGSkDMNwrTMMg7ADAAACisfXoiZNmqQHH3xQpaWl+umnn1RUVOR6HD9+3Bc1AgAAXDSPw87Ro0f12GOPKSIiwhf1AAAAeJXHYadv377Kzc31RS0AAABe5/GcnTvuuENPPPGEPvvsM7Vv315hYWFu6++66y6vFQcAAFBTHoedkSNHSpLmzJlTaZ1hGCovL695VQAAAF7icdj59UfNAQAAAlmN7gz4888/e6uO8zpz5oyefPJJJSQkKDw8XNdcc43mzJnjFrhM05TdbldcXJzCw8OVlJSkAwcO+LQuAABw6fA47JSXl2vu3Lm66qqrdNlll+nbb7+VJM2YMUMvvviiV4tbuHChnn/+eS1btkwHDx7UokWLtHjxYi1dutS1zaJFi7RkyRItW7ZMu3fvVmxsrJKTk1VaWurVWgAAwKXJ47Azf/58rVq1SosWLVLdunVd7e3bt9df/vIXrxb34YcfKjU1VXfccYeaN2+ue+65RykpKa5Pg5mmqYyMDE2fPl1paWlq166dVq9erbKyMmVmZnq1FgAAcGnyeM7OmjVrtGLFCvXp00ePPPKIq71Dhw76/PPPvVrcTTfdpOeff15ffvmlWrZsqY8//ljbt29XRkaGpLPf01VQUKCUlBTXc2w2m2699Vbt3LlTo0aNOu9+HQ6HHA6Ha7mkpESS5HQ65XQ6vXoMgezcsQbTMQcCW6jpn35DTLd/a1uwvs8436gNwfr7vLrH63HYOXr0qFq0aFGpvaKiwusv8pQpU1RcXKzrrrtOoaGhKi8v1/z58/WHP/xBklRQUCBJiomJcXteTEyM8vPzq9zvggULNHv27ErtWVlZQXmzxOzsbH+XEFQWdfNv/3O7+OdDBps2bfJLv/7G+UZtCrbf52VlZdXazuOw07ZtW33wwQdq1qyZW/vf//53de7c2dPdXdCrr76qV155RZmZmWrbtq3y8vI0fvx4xcXFadiwYa7tfvn9XNLZy1u/bvuladOmaeLEia7lkpISxcfHKyUlRVFRUV49hkDmdDqVnZ2t5OTkSvdLgu+0s2/xS7+2EFNzu1RoRm6IHBVV/3z4yn5731rvMxBwvlEbgvX3+bkrM7/F47Aza9Ys3X///Tp69KgqKir0+uuv64svvtCaNWu0ceNGjwu9kCeeeEJTp07VoEGDJJ2dF5Sfn68FCxZo2LBhio2NlXR2hOfKK690Pa+wsLDSaM8v2Ww22Wy2Su1hYWFB9SY5J1iP218c5bX/h8et/wrDLzUE63uM843aFGy/z6t7rB5PUO7fv79effVVbdq0SYZhaObMmTp48KDeeustJScne1zohZSVlSkkxL3E0NBQ10fPExISFBsb6zZsd/r0aeXk5Khnz55erQUAAFyaPB7Zkc5+P1bfvr4fouzfv7/mz5+vpk2bqm3bttq3b5+WLFmiBx98UNLZy1fjx49Xenq6EhMTlZiYqPT0dEVERGjw4ME+rw8AAAS+iwo7tWXp0qWaMWOGRo8ercLCQsXFxWnUqFGaOXOma5vJkyfr1KlTGj16tIqKitS9e3dlZWUpMjLSj5UDAIBAUa2wc/nll19wwu8vHT9+vEYF/VJkZKQyMjJcHzU/H8MwZLfbZbfbvdYvAACwjmqFnV+GjR9//FHz5s1T37591aNHD0lnb/63ZcsWzZgxwydFAgAAXKxqhZ1ffsz797//vebMmaOxY8e62h577DEtW7ZMW7du1YQJE7xfJQAAwEXy+NNYW7Zs0X/8x39Uau/bt6+2bt3qlaIAAAC8xeOw06hRI61fv75S+4YNG9SoUSOvFAUAAOAtHn8aa/bs2RoxYoTef/9915ydXbt2afPmzV7/IlAAAICa8jjsDB8+XK1bt9azzz6r119/XaZpqk2bNtqxY4e6d+/uixoBAAAu2kXdZ6d79+7661//6u1aAAAAvM7jsHP48OELrm/atOlFFwMAAOBtHoed5s2bX/AGg+Xl5TUqCAAAwJs8Djv79u1zW3Y6na7vrJo/f77XCgMAAPAGj8NOx44dK7V16dJFcXFxWrx4sdLS0rxSGAAAl5rmU//hl35toaYWdZPa2bfIUV69r3fypn8+dUet9+kJj++zU5WWLVtq9+7d3todAACAV3g8slNSUuK2bJqmjh07JrvdrsTERK8VBgAA4A0eh52GDRtWmqBsmqbi4+O1du1arxUGAADgDR6Hnffee89tOSQkRFdccYVatGihOnUu6rY9AAAAPuNxOjEMQz179qwUbM6cOaNt27bplltu8VpxAAAANeXxBOXevXvr+PHjldqLi4vVu3dvrxQFAADgLR6HHdM0z3tTwR9//FH169f3SlEAAADeUu3LWOfun2MYhoYPHy6bzeZaV15erk8++UQ9e/b0foUAAAA1UO2w06BBA0lnR3YiIyMVHh7uWle3bl3deOONGjlypPcrBAAAqIFqh52VK1dKOvvdWI8//jiXrAAAwCXB4zk7kydPdpuzk5+fr4yMDGVlZXm1MAAAAG/wOOykpqZqzZo1kqSffvpJ3bp109NPP63U1FQtX77c6wUCAADUhMdhZ+/evbr55pslSa+99ppiY2OVn5+vNWvW6Nlnn/V6gQAAADXhcdgpKytTZGSkJCkrK0tpaWkKCQnRjTfeqPz8fK8XCAAAUBMeh50WLVpow4YNOnLkiLZs2aKUlBRJUmFhoaKiorxeIAAAQE14HHZmzpypxx9/XM2bN1f37t3Vo0cPSWdHeTp37uz1AgEAAGrC4+/Guueee3TTTTfp2LFj6tixo6u9T58+uvvuu71aHAAAQE1d1NeUx8bGKjY21q2tW7duXikIAADAmzy+jAUAAHApIewAAABLI+wAAABLq1bYuf7661VUVCRJmjNnjsrKynxaFAAAgLdUK+wcPHhQJ0+elCTNnj1bJ06c8GlRAAAA3lKtT2N16tRJDzzwgG666SaZpqn/+Z//0WWXXXbebWfOnOnVAgEAAGqiWmFn1apVmjVrljZu3CjDMPT222+rTp3KTzUMg7ADAAACSrXCTqtWrbR27VpJUkhIiN555x01adLEp4UBAAB4g8c3FayoqPBFHQAAAD5xUXdQ/uabb5SRkaGDBw/KMAy1bt1a48aN07XXXuvt+gAAAGrE4/vsbNmyRW3atNFHH32kDh06qF27dvrf//1ftW3bVtnZ2b6oEQAA4KJ5PLIzdepUTZgwQU899VSl9ilTpig5OdlrxQEAANSUxyM7Bw8e1IgRIyq1P/jgg/rss8+8UhQAAIC3eBx2rrjiCuXl5VVqz8vL4xNaAAAg4Hh8GWvkyJF6+OGH9e2336pnz54yDEPbt2/XwoULNWnSJF/UCAAAcNE8DjszZsxQZGSknn76aU2bNk2SFBcXJ7vdrscee8zrBQIAANSEx2HHMAxNmDBBEyZMUGlpqSQpMjLS64UBAAB4g8dzdn4pMjLS50Hn6NGjuu+++9SoUSNFRESoU6dO2rNnj2u9aZqy2+2Ki4tTeHi4kpKSdODAAZ/WBAAALh01Cju+VlRUpF69eiksLExvv/22PvvsMz399NNq2LCha5tFixZpyZIlWrZsmXbv3q3Y2FglJye7Rp0AAEBwu6g7KNeWhQsXKj4+XitXrnS1NW/e3PV/0zSVkZGh6dOnKy0tTZK0evVqxcTEKDMzU6NGjTrvfh0OhxwOh2u5pKREkuR0OuV0On1wJIHp3LEG0zEHAluo6Z9+Q0y3f2tbsL7PON/BhfMdmP0apmn655WphjZt2qhv37767rvvlJOTo6uuukqjR4/WyJEjJUnffvutrr32Wu3du1edO3d2PS81NVUNGzbU6tWrz7tfu92u2bNnV2rPzMxURESEbw4GAAB4VVlZmQYPHqzi4mJFRUVVuZ1HYcfpdColJUUvvPCCWrZs6ZVCL6RevXqSpIkTJ+o///M/9dFHH2n8+PF64YUXNHToUO3cuVO9evXS0aNHFRcX53reww8/rPz8fG3ZsuW8+z3fyE58fLx++OGHC75YVuN0OpWdna3k5GSFhYX5u5yg0c5+/velr9lCTM3tUqEZuSFyVBi13v9+e99a7zMQcL6DC+e7dpWUlKhx48a/GXY8uowVFham/fv3yzBq54WsqKhQly5dlJ6eLknq3LmzDhw4oOXLl2vo0KGu7X5dj2maF6zRZrPJZrNVag8LCwvKP/rBetz+4iiv/V9Ebv1XGH6pIVjfY5zv4ML5Dsx+PZ6gPHToUL344oseF3QxrrzySrVp08atrXXr1jp8+LAkKTY2VpJUUFDgtk1hYaFiYmJqpUYAABDYPJ6gfPr0af3lL39Rdna2unTpovr167utX7JkideK69Wrl7744gu3ti+//FLNmjWTJCUkJCg2NlbZ2dmuOTunT59WTk6OFi5c6LU6AADApcvjsLN//35df/31ks4Gj1/y9uWtCRMmqGfPnkpPT9fAgQP10UcfacWKFVqxYoWrv/Hjxys9PV2JiYlKTExUenq6IiIiNHjwYK/WAgAALk0eh5333nvPF3WcV9euXbV+/XpNmzZNc+bMUUJCgjIyMjRkyBDXNpMnT9apU6c0evRoFRUVqXv37srKyuKuzgAAQFIN7rPz9ddf65tvvtEtt9yi8PDw35wUfLHuvPNO3XnnnVWuNwxDdrtddrvd630DAIBLn8cTlH/88Uf16dNHLVu21O9+9zsdO3ZMkvTQQw/xrecAACDgeBx2JkyYoLCwMB0+fNjtBnz33nuvNm/e7NXiAAAAasrjy1hZWVnasmWLrr76arf2xMRE5efne60wAAAAb/B4ZOfkyZPn/UqFH3744bw36gMAAPAnj8POLbfcojVr1riWDcNQRUWFFi9erN69e3u1OAAAgJry+DLW4sWLlZSUpNzcXJ0+fVqTJ0/WgQMHdPz4ce3YscMXNQIAAFw0j0d22rRpo08++UTdunVTcnKyTp48qbS0NO3bt0/XXnutL2oEAAC4aBd1n53Y2FjNnj3b27UAAAB43UWFnaKiIr344os6ePCgDMNQ69at9cADDyg6Otrb9QEAANSIx5excnJylJCQoGeffVZFRUU6fvy4nn32WSUkJCgnJ8cXNQIAAFw0j0d2xowZo4EDB2r58uUKDQ2VJJWXl2v06NEaM2aM9u/f7/UiAQAALpbHIzvffPONJk2a5Ao6khQaGqqJEyfqm2++8WpxAAAANeVx2Ln++ut18ODBSu0HDx5Up06dvFETAACA11TrMtYnn3zi+v9jjz2mcePG6euvv9aNN94oSdq1a5f+9Kc/6amnnvJNlQAAABepWmGnU6dOMgxDpmm62iZPnlxpu8GDB+vee+/1XnUAAAA1VK2wc+jQIV/XAQAA4BPVCjvNmjXzdR0AAAA+cVE3FTx69Kh27NihwsJCVVRUuK177LHHvFIYAACAN3gcdlauXKlHHnlEdevWVaNGjWQYhmudYRiEHQAAEFA8DjszZ87UzJkzNW3aNIWEePzJdQAAgFrlcVopKyvToEGDCDoAAOCS4HFiGTFihP7+97/7ohYAAACv8/gy1oIFC3TnnXdq8+bNat++vcLCwtzWL1myxGvFAQAA1JTHYSc9PV1btmxRq1atJKnSBGUAAIBA4nHYWbJkiV566SUNHz7cB+UAAAB4l8dzdmw2m3r16uWLWgAAALzO47Azbtw4LV261Be1AAAAeJ3Hl7E++ugjvfvuu9q4caPatm1baYLy66+/7rXiAAAAasrjsNOwYUOlpaX5ohYAAACvu6iviwAAALhUcBtkAABgaR6P7CQkJFzwfjrffvttjQoCAADwJo/Dzvjx492WnU6n9u3bp82bN+uJJ57wVl0AAABe4XHYGTdu3Hnb//SnPyk3N7fGBQEAAHiT1+bs9OvXT+vWrfPW7gAAALzCa2HntddeU3R0tLd2BwAA4BUeX8bq3Lmz2wRl0zRVUFCgf//733ruuee8WhwAAEBNeRx2BgwY4LYcEhKiK664QklJSbruuuu8VRcAAIBXeBx2Zs2a5Ys6AAAAfIKbCgIAAEur9shOSEjIBW8mKEmGYejMmTM1LgoAAMBbqh121q9fX+W6nTt3aunSpTJN0ytFAQAAeEu1w05qamqlts8//1zTpk3TW2+9pSFDhmju3LleLQ4AAKCmLmrOzvfff6+RI0eqQ4cOOnPmjPbt26fVq1eradOm3q4PAACgRjwKO8XFxZoyZYpatGihAwcO6J133tFbb72l9u3b+6o+AACAGqn2ZaxFixZp4cKFio2N1d/+9rfzXtYCAAAINNUe2Zk6dap+/vlntWjRQqtXr1ZaWtp5H760YMECGYbh9s3rpmnKbrcrLi5O4eHhSkpK0oEDB3xaBwAAuHRUe2Rn6NChv/nRc1/avXu3VqxYoQ4dOri1L1q0SEuWLNGqVavUsmVLzZs3T8nJyfriiy8UGRnpp2oBAECgqHbYWbVqlQ/LuLATJ05oyJAh+vOf/6x58+a52k3TVEZGhqZPn+4aVVq9erViYmKUmZmpUaNGnXd/DodDDofDtVxSUiJJcjqdcjqdPjySwHLuWIPpmAOBLdQ/t2iwhZhu/9a2YH2fcb6DC+c7MPs1zEvg5jjDhg1TdHS0nnnmGSUlJalTp07KyMjQt99+q2uvvVZ79+5V586dXdunpqaqYcOGWr169Xn3Z7fbNXv27ErtmZmZioiI8NlxAAAA7ykrK9PgwYNVXFysqKioKrfz+LuxatvatWu1Z88e5ebmVlpXUFAgSYqJiXFrj4mJUX5+fpX7nDZtmiZOnOhaLikpUXx8vFJSUi74YlmN0+lUdna2kpOTFRYW5u9ygkY7+xa/9GsLMTW3S4Vm5IbIUVH7l6T32/vWep+BgPMdXDjftevclZnfEtBh58iRIxo3bpyysrJUr169Krf79Vwi0zQvOL/IZrPJZrNVag8LCwvKP/rBetz+4ij339w3SXJUGH6pIVjfY5zv4ML5Dsx+A/qLQPfs2aPCwkLdcMMNqlOnjurUqaOcnBw9++yzqlOnjmtE59wIzzmFhYWVRnsAAEBwCuiw06dPH3366afKy8tzPbp06aIhQ4YoLy9P11xzjWJjY5Wdne16zunTp5WTk6OePXv6sXIAABAoAvoyVmRkpNq1a+fWVr9+fTVq1MjVPn78eKWnpysxMVGJiYlKT09XRESEBg8e7I+SAQBAgAnosFMdkydP1qlTpzR69GgVFRWpe/fuysrK4h47AABA0iUYdt5//323ZcMwZLfbZbfb/VIPAAAIbAE9ZwcAAKCmLrmRHStqPvUffunXFmpqUbez94Xwx0cV//nUHbXeJwAg+DCyAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALC2gw86CBQvUtWtXRUZGqkmTJhowYIC++OILt21M05TdbldcXJzCw8OVlJSkAwcO+KliAAAQaAI67OTk5GjMmDHatWuXsrOzdebMGaWkpOjkyZOubRYtWqQlS5Zo2bJl2r17t2JjY5WcnKzS0lI/Vg4AAAJFHX8XcCGbN292W165cqWaNGmiPXv26JZbbpFpmsrIyND06dOVlpYmSVq9erViYmKUmZmpUaNG+aNsAAAQQAI67PxacXGxJCk6OlqSdOjQIRUUFCglJcW1jc1m06233qqdO3dWGXYcDoccDodruaSkRJLkdDrldDp9VX6VbKFmrfcpSbYQ0+3f2uaP1zoQcL6DC+c7uHC+A7NfwzRN/7wyHjJNU6mpqSoqKtIHH3wgSdq5c6d69eqlo0ePKi4uzrXtww8/rPz8fG3ZsuW8+7Lb7Zo9e3al9szMTEVERPjmAAAAgFeVlZVp8ODBKi4uVlRUVJXbXTIjO2PHjtUnn3yi7du3V1pnGIbbsmmaldp+adq0aZo4caJruaSkRPHx8UpJSbngi+Ur7eznD2W+ZgsxNbdLhWbkhshRUfXr5Sv77X1rvc9AwPkOLpzv4ML5rl3nrsz8lksi7Dz66KN68803tW3bNl199dWu9tjYWElSQUGBrrzySld7YWGhYmJiqtyfzWaTzWar1B4WFqawsDAvVl49jvLaf2O69V9h+KUGf7zWgYDzHVw438GF8x2Y/Qb0p7FM09TYsWP1+uuv691331VCQoLb+oSEBMXGxio7O9vVdvr0aeXk5Khnz561XS4AAAhAAT2yM2bMGGVmZuqNN95QZGSkCgoKJEkNGjRQeHi4DMPQ+PHjlZ6ersTERCUmJio9PV0REREaPHiwn6sHAACBIKDDzvLlyyVJSUlJbu0rV67U8OHDJUmTJ0/WqVOnNHr0aBUVFal79+7KyspSZGRkLVcLAAACUUCHnep8UMwwDNntdtntdt8XBAAALjkBPWcHAACgpgg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0iwTdp577jklJCSoXr16uuGGG/TBBx/4uyQAABAALBF2Xn31VY0fP17Tp0/Xvn37dPPNN6tfv346fPiwv0sDAAB+Zomws2TJEo0YMUIPPfSQWrdurYyMDMXHx2v58uX+Lg0AAPhZHX8XUFOnT5/Wnj17NHXqVLf2lJQU7dy587zPcTgccjgcruXi4mJJ0vHjx+V0On1XbBXqnDlZ631KUp0KU2VlFarjDFF5hVHr/f/444+13mcg4HwHF853cOF8167S0lJJkmmaF97QvMQdPXrUlGTu2LHDrX3+/Plmy5Ytz/ucWbNmmZJ48ODBgwcPHhZ4HDly5IJZ4ZIf2TnHMNyTrGmaldrOmTZtmiZOnOharqio0PHjx9WoUaMqn2NFJSUlio+P15EjRxQVFeXvcuBjnO/gwvkOLsF6vk3TVGlpqeLi4i643SUfdho3bqzQ0FAVFBS4tRcWFiomJua8z7HZbLLZbG5tDRs29FWJAS8qKiqofjiCHec7uHC+g0swnu8GDRr85jaX/ATlunXr6oYbblB2drZbe3Z2tnr27OmnqgAAQKC45Ed2JGnixIm6//771aVLF/Xo0UMrVqzQ4cOH9cgjj/i7NAAA4GeWCDv33nuvfvzxR82ZM0fHjh1Tu3bttGnTJjVr1szfpQU0m82mWbNmVbqkB2vifAcXzndw4XxfmGGav/V5LQAAgEvXJT9nBwAA4EIIOwAAwNIIOwAAwNIIOwAAwNIIO0Fo27Zt6t+/v+Li4mQYhjZs2ODvkuAjCxYsUNeuXRUZGakmTZpowIAB+uKLL/xdFnxo+fLl6tChg+vmcj169NDbb7/t77JQCxYsWCDDMDR+/Hh/lxJwCDtB6OTJk+rYsaOWLVvm71LgYzk5ORozZox27dql7OxsnTlzRikpKTp50j9fVgjfu/rqq/XUU08pNzdXubm5uu2225SamqoDBw74uzT40O7du7VixQp16NDB36UEJD56HuQMw9D69es1YMAAf5eCWvDvf/9bTZo0UU5Ojm655RZ/l4NaEh0drcWLF2vEiBH+LgU+cOLECV1//fV67rnnNG/ePHXq1EkZGRn+LiugMLIDBJHi4mJJZ//4wfrKy8u1du1anTx5Uj169PB3OfCRMWPG6I477tDtt9/u71ICliXuoAzgt5mmqYkTJ+qmm25Su3bt/F0OfOjTTz9Vjx499PPPP+uyyy7T+vXr1aZNG3+XBR9Yu3at9uzZo9zcXH+XEtAIO0CQGDt2rD755BNt377d36XAx1q1aqW8vDz99NNPWrdunYYNG6acnBwCj8UcOXJE48aNU1ZWlurVq+fvcgIac3aCHHN2gsOjjz6qDRs2aNu2bUpISPB3Oahlt99+u6699lq98MIL/i4FXrRhwwbdfffdCg0NdbWVl5fLMAyFhITI4XC4rQtmjOwAFmaaph599FGtX79e77//PkEnSJmmKYfD4e8y4GV9+vTRp59+6tb2wAMP6LrrrtOUKVMIOr9A2AlCJ06c0Ndff+1aPnTokPLy8hQdHa2mTZv6sTJ425gxY5SZmak33nhDkZGRKigokCQ1aNBA4eHhfq4OvvDf//3f6tevn+Lj41VaWqq1a9fq/fff1+bNm/1dGrwsMjKy0vy7+vXrq1GjRszL+xXCThDKzc1V7969XcsTJ06UJA0bNkyrVq3yU1XwheXLl0uSkpKS3NpXrlyp4cOH135B8Ll//etfuv/++3Xs2DE1aNBAHTp00ObNm5WcnOzv0gC/Yc4OAACwNO6zAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wA6Dahg8frgEDBlS5/tSpU5o1a5ZatWolm82mxo0b65577tGBAwfctrPb7TIMw/XtzHFxcRoyZIiOHDni0/rff/99GYahn376yaf9AAgshB0AXuFwOHT77bfrpZde0ty5c/Xll19q06ZNKi8vV/fu3bVr1y637du2batjx47pu+++06uvvqpPP/1UAwcO9FP1/nH69Gl/lwAEBcIOAK/IyMjQhx9+qI0bN2rgwIFq1qyZunXrpnXr1ql169YaMWKEfvlVfHXq1FFsbKzi4uJ08803a+TIkdq1a5dKSkou2M+bb76pLl26qF69emrcuLHS0tJc61555RV16dJFkZGRio2N1eDBg1VYWChJ+uc//+n6AtzLL79chmG4vgzVNE0tWrRI11xzjcLDw9WxY0e99tprlfpNTExUeHi4evfurdWrV1caJVq3bp3atm0rm82m5s2b6+mnn3bbR/PmzTVv3jwNHz5cDRo00MiRI3Xbbbdp7Nixbtv9+OOPstlsevfdd6v34gO4IMIOAK/IzMxUcnKyOnbs6NYeEhKiCRMm6LPPPtPHH3983ucWFBTo9ddfV2hoqEJDQ6vs4x//+IfS0tJ0xx13aN++fXrnnXfUpUsX1/rTp09r7ty5+vjjj7VhwwYdOnTIFWji4+O1bt06SdIXX3yhY8eO6Y9//KMk6cknn9TKlSu1fPlyHThwQBMmTNB9992nnJwcSWeD0j333KMBAwYoLy9Po0aN0vTp091q27NnjwYOHKhBgwbp008/ld1u14wZM7Rq1Sq37RYvXqx27dppz549mjFjhh566CFlZmbK4XC4tvnrX/+quLg4VzgDUEMmAFTTsGHDzNTU1POuq1evnjlu3Ljzrtu7d68pyXz11VdN0zTNWbNmmSEhIWb9+vXN8PBwU5IpyXzssccu2H+PHj3MIUOGVLvejz76yJRklpaWmqZpmu+9954pySwqKnJtc+LECbNevXrmzp073Z47YsQI8w9/+INpmqY5ZcoUs127dm7rp0+f7ravwYMHm8nJyW7bPPHEE2abNm1cy82aNTMHDBjgts3PP/9sRkdHu14b0zTNTp06mXa7vdrHCeDCGNkB4HPm/7t8ZRiGq61Vq1bKy8vT7t27NX/+fHXq1Enz58+/4H7y8vLUp0+fKtfv27dPqampatasmSIjI5WUlCRJOnz4cJXP+eyzz/Tzzz8rOTlZl112meuxZs0affPNN5LOjgR17drV7XndunVzWz548KB69erl1tarVy999dVXKi8vd7X9ciRKkmw2m+677z699NJLrmP8+OOPXSNSAGqujr8LAGANLVu21GeffXbedZ9//rkkKTEx0dVWt25dtWjRQtLZycpfffWV/uu//ksvv/xylX2Eh4dXue7kyZNKSUlRSkqKXnnlFV1xxRU6fPiw+vbte8GJwBUVFZLOXiK76qqr3NbZbDZJZ8PaL4PaubZfL//WNpJUv379Sm0PPfSQOnXqpO+++04vvfSS+vTpo2bNmlVZMwDPMLIDwCsGDRqkrVu3VpqXU1FRoWeeeUZt2rSpNJ/nl2bMmKG//e1v2rt3b5XbdOjQQe+88855133++ef64Ycf9NRTT+nmm2/Wdddd55qcfE7dunUlyW2kpU2bNrLZbDp8+LBatGjh9oiPj5ckXXfdddq9e7fbvnJzc92W27Rpo+3bt7u17dy5Uy1btrzgPCRJat++vbp06aI///nPyszM1IMPPnjB7QF4hpEdAB4pLi5WXl6eW1t0dLQmTJigN954Q/3799fTTz+t7t2761//+pfS09N18OBBbd26tdLIxy9dc801Sk1N1cyZM7Vx48bzbjNr1iz16dNH1157rQYNGqQzZ87o7bff1uTJk9W0aVPVrVtXS5cu1SOPPKL9+/dr7ty5bs9v1qyZDMPQxo0b9bvf/U7h4eGKjIzU448/rgkTJqiiokI33XSTSkpKtHPnTl122WUaNmyYRo0apSVLlmjKlCkaMWKE8vLyXBOPzx3TpEmT1LVrV82dO1f33nuvPvzwQy1btkzPPfdctV7Xhx56SGPHjlVERITuvvvuaj0HQDX5dcYQgEvKsGHDXJOJf/kYNmyYaZqmefLkSfPJJ580W7RoYYaFhZnR0dHm73//e/PTTz9128+sWbPMjh07Vtr/jh07TEnmrl27qqxh3bp1ZqdOncy6deuajRs3NtPS0lzrMjMzzebNm5s2m83s0aOH+eabb5qSzH379rm2mTNnjhkbG2sahuGqu6KiwvzjH/9otmrVygwLCzOvuOIKs2/fvmZOTo7reW+88YbZokUL02azmUlJSeby5ctNSeapU6dc27z22mtmmzZtzLCwMLNp06bm4sWL3Wpv1qyZ+cwzz5z3uEpLS82IiAhz9OjRVR47gItjmOZ5LioDAC5o/vz5ev7557121+cjR46oefPm2r17t66//nqv7BPAWVzGAoBqeO6559S1a1c1atRIO3bs0OLFiyvdDPBiOJ1OHTt2TFOnTtWNN95I0AF8gLADANXw1Vdfad68eTp+/LiaNm2qSZMmadq0aTXe744dO9S7d2+1bNmy0l2bAXgHl7EAAICl8dFzAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaf8HuSXTpZvy3ZMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stratifying the split with respect to LOR rating to avoid sampling bias\n",
    "# Step 1: Checking LOR rating distribution to make sure size of strata doesn't skew results \n",
    "X['LOR_cat'] = pd.cut(X['LOR'], bins = [1, 2, 3, 4, 5], labels = [1, 2, 3, 4])\n",
    "\n",
    "#Step 2: Visualizing LOR_cat distribution\n",
    "X['LOR_cat'].value_counts().sort_index().plot.bar(rot = 0, grid = True)\n",
    "plt.xlabel('LOR category')\n",
    "plt.ylabel('Number of students')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b91f14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%-20% Train-Val split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,stratify = X['LOR_cat'], random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fd7894d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% POP</th>\n",
       "      <th>% VAL</th>\n",
       "      <th>% Comp Error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.107872</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>-0.059538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.314869</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.361516</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.002221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.215743</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.007638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 % POP     % VAL  % Comp Error\n",
       "LOR Category                                  \n",
       "1             0.107872  0.101449     -0.059538\n",
       "2             0.314869  0.318841      0.012614\n",
       "3             0.361516  0.362319      0.002221\n",
       "4             0.215743  0.217391      0.007638"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing composition of Val set with that of population (X)\n",
    "val_comp = X_val['LOR_cat'].value_counts().sort_index()/len(X_val)\n",
    "pop_comp = X['LOR_cat'].value_counts().sort_index()/len(X)\n",
    "comp = pd.DataFrame({'% POP':pop_comp, '% VAL':val_comp}, index = [1,2,3,4])\n",
    "comp.index.name = 'LOR Category'\n",
    "comp['% Comp Error'] = val_comp/pop_comp - 1\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec7539",
   "metadata": {},
   "source": [
    "As can be seen in the table above, the composition of the validation set (sample) is similar to that of the original training set (population), with negligible composition error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f13496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop LOR_cat from sets (wont be required)\n",
    "X_train = X_train.drop('LOR_cat', axis = 1).reset_index(drop = True)\n",
    "X_val = X_val.drop('LOR_cat', axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8950340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_Score</th>\n",
       "      <th>TOEFL_Score</th>\n",
       "      <th>University_Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>336</td>\n",
       "      <td>118</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>309</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333</td>\n",
       "      <td>118</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>327</td>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>313</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>312</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>296</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>298</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE_Score  TOEFL_Score  University_Rating  SOP  LOR  CGPA  Research\n",
       "0          304          105                  2  3.0  3.0  8.20         1\n",
       "1          333          119                  5  5.0  4.5  9.78         1\n",
       "2          336          118                  5  4.5  5.0  9.53         1\n",
       "3          309          104                  2  2.0  2.5  8.26         0\n",
       "4          333          118                  5  5.0  5.0  9.35         1\n",
       "..         ...          ...                ...  ...  ...   ...       ...\n",
       "269        327          104                  5  3.0  3.5  8.84         1\n",
       "270        313          107                  3  4.0  4.5  8.69         0\n",
       "271        312          105                  2  2.0  2.5  8.45         0\n",
       "272        296           99                  2  3.0  3.5  7.28         0\n",
       "273        298           99                  1  1.5  3.0  7.46         0\n",
       "\n",
       "[274 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b104420",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e596d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing based on training set\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardization applies only to numerical variables. Drop 'Research' and join after standardization\n",
    "res_train = X_train['Research']\n",
    "res_val = X_val['Research']\n",
    "X_train = X_train.drop('Research', axis = 1)\n",
    "X_val = X_val.drop('Research', axis = 1)\n",
    "\n",
    "# Standardize training set\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_train_std = pd.DataFrame(X_train_std, columns = X_train.columns)\n",
    "#Join 'Research' to training and std_training sets\n",
    "X_train['Research'] = res_train\n",
    "X_train_std['Research'] = res_train\n",
    "\n",
    "# Standardize val set\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_val_std = pd.DataFrame(X_val_std, columns = X_val.columns)\n",
    "#Join 'Research' to training and std_training sets\n",
    "X_val['Research'] = res_val\n",
    "X_val_std['Research'] = res_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "050d784c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_Score</th>\n",
       "      <th>TOEFL_Score</th>\n",
       "      <th>University_Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.289903</td>\n",
       "      <td>1.721007</td>\n",
       "      <td>1.597354</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>1.694715</td>\n",
       "      <td>1.760335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.112432</td>\n",
       "      <td>1.203950</td>\n",
       "      <td>1.597354</td>\n",
       "      <td>1.067335</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>1.233062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.313813</td>\n",
       "      <td>0.169836</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.283971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.460897</td>\n",
       "      <td>-0.691925</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>-0.911182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.023697</td>\n",
       "      <td>0.859245</td>\n",
       "      <td>1.597354</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>1.136600</td>\n",
       "      <td>1.426396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.928484</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>0.557841</td>\n",
       "      <td>1.136600</td>\n",
       "      <td>-0.471788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-1.638368</td>\n",
       "      <td>-1.381334</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-1.333000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.307336</td>\n",
       "      <td>-0.174868</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>-0.137848</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.201167</td>\n",
       "      <td>1.376302</td>\n",
       "      <td>1.597354</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>1.694715</td>\n",
       "      <td>1.303365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1.017219</td>\n",
       "      <td>-1.381334</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>-1.122091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GRE_Score  TOEFL_Score  University_Rating       SOP       LOR      CGPA  \\\n",
       "0    1.289903     1.721007           1.597354  1.576829  1.694715  1.760335   \n",
       "1    1.112432     1.203950           1.597354  1.067335 -0.537746  1.233062   \n",
       "2    0.313813     0.169836          -0.153346  0.048346  0.020369  0.283971   \n",
       "3   -1.460897    -0.691925          -0.153346  0.048346  0.578485 -0.911182   \n",
       "4    1.023697     0.859245           1.597354  1.576829  1.136600  1.426396   \n",
       "..        ...          ...                ...       ...       ...       ...   \n",
       "64  -0.928484     0.342188           0.722004  0.557841  1.136600 -0.471788   \n",
       "65  -1.638368    -1.381334          -1.028696 -0.461148  0.020369 -1.333000   \n",
       "66  -0.307336    -0.174868          -1.028696 -0.970643  0.578485 -0.137848   \n",
       "67   1.201167     1.376302           1.597354  1.576829  1.694715  1.303365   \n",
       "68  -1.017219    -1.381334          -1.028696 -0.461148 -0.537746 -1.122091   \n",
       "\n",
       "    Research  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "..       ...  \n",
       "64         0  \n",
       "65         0  \n",
       "66         0  \n",
       "67         1  \n",
       "68         0  \n",
       "\n",
       "[69 rows x 7 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477f3f7",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7f0eb282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression intercept is 0.7241812528192324\n",
      "\n",
      "Estimated regression parameters:\n",
      "            Variables      Beta\n",
      "0          GRE_Score  0.023699\n",
      "1        TOEFL_Score  0.012982\n",
      "2  University_Rating  0.011583\n",
      "3                SOP -0.000482\n",
      "4                LOR  0.016472\n",
      "5               CGPA  0.062939\n",
      "6           Research  0.014257\n",
      "\n",
      "The RMSE of the LR model is 0.0686\n"
     ]
    }
   ],
   "source": [
    "# Building and fitting model to training set (Std data)\n",
    "lr = LinearRegression() \n",
    "model = lr.fit(X_train_std,y_train) \n",
    "\n",
    "print(f'The regression intercept is {model.intercept_}')\n",
    "print(f\"\\nEstimated regression parameters:\\n {pd.DataFrame({'Variables':X_train.columns, 'Beta':model.coef_})}\")\n",
    "\n",
    "# Generating and testing predictions\n",
    "y_val_pred = model.predict(X_val_std)\n",
    "mse_lr = mean_squared_error(y_val,y_val_pred)\n",
    "print(f'\\nThe RMSE of the LR model is {np.sqrt(mse_lr):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb5c2713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression intercept is 0.42195788913852184\n",
      "\n",
      "Estimated regression parameters:\n",
      "            Variables      Beta\n",
      "0          GRE_Score  0.105147\n",
      "1        TOEFL_Score  0.060411\n",
      "2  University_Rating  0.040558\n",
      "3                SOP -0.001965\n",
      "4                LOR  0.064351\n",
      "5               CGPA  0.294248\n",
      "6           Research  0.014257\n",
      "\n",
      "The RMSE of the LR model is 0.0686\n"
     ]
    }
   ],
   "source": [
    "# Building and fitting model to training set (Normalized data)\n",
    "lr = LinearRegression() \n",
    "model = lr.fit(X_train_norm,y_train) \n",
    "\n",
    "print(f'The regression intercept is {model.intercept_}')\n",
    "print(f\"\\nEstimated regression parameters:\\n {pd.DataFrame({'Variables':X_train.columns, 'Beta':model.coef_})}\")\n",
    "\n",
    "# Generating and testing predictions\n",
    "y_val_pred = model.predict(X_val_norm)\n",
    "mse_lr = mean_squared_error(y_val,y_val_pred)\n",
    "print(f'\\nThe RMSE of the LR model is {np.sqrt(mse_lr):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b03f25",
   "metadata": {},
   "source": [
    "The performance of the LR model on botht the normalized and the standardized data sets is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279b185",
   "metadata": {},
   "source": [
    "## LASSO Regression (Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b9d8dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m.maraqa\\AppData\\Local\\Temp\\ipykernel_22360\\3934483825.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model = lasso.fit(X_train_std,y_train)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.075e-01, tolerance: 4.918e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.949e-01, tolerance: 4.138e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.796e-01, tolerance: 4.535e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.795e-01, tolerance: 4.509e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e-01, tolerance: 4.463e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.414e-01, tolerance: 4.224e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.626e-01, tolerance: 4.458e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e-01, tolerance: 4.440e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.746e-01, tolerance: 4.564e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e-01, tolerance: 4.422e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e-01, tolerance: 4.491e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal alpha value is 0.0\n"
     ]
    }
   ],
   "source": [
    "# Finding optimal alpha value (std data)\n",
    "alps = []\n",
    "cv_scores = []\n",
    "for i in np.arange(0,1,0.01):\n",
    "    alps.append(i)\n",
    "    lasso = Lasso(alpha = i, random_state = 5)\n",
    "    model = lasso.fit(X_train_std,y_train)\n",
    "    scores = -cross_val_score(lasso, X_train_std, y_train, cv = 10, scoring = 'neg_mean_squared_error')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "optimal_alp = alps[cv_scores.index(min(cv_scores))]\n",
    "print(f'The optimal alpha value is {optimal_alp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6ebedc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m.maraqa\\AppData\\Local\\Temp\\ipykernel_22360\\1593691696.py:7: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model = lasso.fit(X_train_std,y_train)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.075e-01, tolerance: 4.918e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.949e-01, tolerance: 4.138e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.796e-01, tolerance: 4.535e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.795e-01, tolerance: 4.509e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e-01, tolerance: 4.463e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.414e-01, tolerance: 4.224e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.626e-01, tolerance: 4.458e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e-01, tolerance: 4.440e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.746e-01, tolerance: 4.564e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.504e-01, tolerance: 4.422e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\m.maraqa\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e-01, tolerance: 4.491e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal alpha value is 0.0\n"
     ]
    }
   ],
   "source": [
    "# Finding optimal alpha value (Norm data)\n",
    "alps = []\n",
    "cv_scores = []\n",
    "for i in np.arange(0,1,0.01):\n",
    "    alps.append(i)\n",
    "    lasso = Lasso(alpha = i, random_state = 5)\n",
    "    model = lasso.fit(X_train_std,y_train)\n",
    "    scores = -cross_val_score(lasso, X_train_norm, y_train, cv = 10, scoring = 'neg_mean_squared_error')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "optimal_alp = alps[cv_scores.index(min(cv_scores))]\n",
    "print(f'The optimal alpha value is {optimal_alp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ee504",
   "metadata": {},
   "source": [
    "Since the results of the cross validation of the LASSO regression show that the optimal alpha value is 0, the simple linear model performs better any other LASSO model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb32505",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c617be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal alpha value is 70\n"
     ]
    }
   ],
   "source": [
    "alps = []\n",
    "cv_scores = []\n",
    "for i in range(40,80,10):\n",
    "    alps.append(i)\n",
    "    rdg = Ridge(alpha=i)\n",
    "    model = rdg.fit(X_train_std,y_train)\n",
    "    scores = -cross_val_score(estimator = model, X = X_train_std, y = y_train, cv = 10, scoring = 'neg_mean_squared_error')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "optimal_alp = alps[cv_scores.index(max(cv_scores))]\n",
    "print(f'The optimal alpha value is {optimal_alp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cfeb5d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The RMSE of the Ridge Regression model is 0.0706\n"
     ]
    }
   ],
   "source": [
    "opt_rdg = Ridge(alpha = 70)\n",
    "model = opt_rdg.fit(X_train_std, y_train)\n",
    "\n",
    "# Generating and testing predictions\n",
    "y_val_pred = model.predict(X_val_std)\n",
    "mse_rdg = mean_squared_error(y_val,y_val_pred)\n",
    "print(f'\\nThe RMSE of the Ridge Regression model is {np.sqrt(mse_rdg):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9440354",
   "metadata": {},
   "source": [
    "The ridge regression performed worse than the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff67462",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ddb6b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1080 candidates, totalling 5400 fits\n",
      "Best parameter combination for RF model is\n",
      " {'max_depth': 4, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 6}\n"
     ]
    }
   ],
   "source": [
    "# Finding the optimal hyperparameters\n",
    "rf_ps = {'min_samples_split': list(range(2,5,1)),\n",
    "         'min_samples_leaf': list(range(1,5,1)),\n",
    "         'max_depth': list(range(1,6)),\n",
    "         'n_estimators': list(range(1,7)),\n",
    "         'max_features':['auto', 'log2' ,'sqrt']}\n",
    "\n",
    "rf=RandomForestRegressor(random_state=0)\n",
    "gcv_rf = GridSearchCV(estimator = rf, param_grid = rf_ps, cv = 5, n_jobs = -1, verbose = True, scoring = 'neg_mean_squared_error')\n",
    "gcv_rf.fit(X_train_std, y_train)\n",
    "print('Best parameter combination for RF model is\\n', gcv_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "993ab581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The RMSE of the Random Forest model is 0.0722\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(max_depth = 4, max_features = 'log2', min_samples_leaf = 3, min_samples_split = 2, n_estimators = 6, random_state=0)\n",
    "model = rf.fit(X_train_std, y_train)\n",
    "\n",
    "# Generating and testing predictions\n",
    "y_val_pred = model.predict(X_val_std)\n",
    "mse_rf = mean_squared_error(y_val,y_val_pred)\n",
    "print(f'\\nThe RMSE of the Random Forest model is {np.sqrt(mse_rf):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8661da",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6c093601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3456 candidates, totalling 17280 fits\n",
      "Best parameter combination for GBT model is\n",
      " {'learning_rate': 0.1, 'max_depth': 4, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 6}\n"
     ]
    }
   ],
   "source": [
    "# Finding optimal hyperparameters\n",
    "gbt_ps = {'learning_rate': list(i for i in np.arange(0.025, 0.125, 0.025)),\n",
    "          'min_samples_split': list(range(2,5,1)),\n",
    "          'min_samples_leaf': list(range(1,5,1)),\n",
    "          'max_depth': list(range(2,6)),\n",
    "          'n_estimators': list(range(1,7,1)),\n",
    "          'max_features':['auto', 'log2' ,'sqrt']}\n",
    "\n",
    "gbt = GradientBoostingRegressor(random_state = 0)\n",
    "gcv_gbt = GridSearchCV(estimator = gbt, param_grid = gbt_ps, cv = 5, n_jobs = -1, verbose = True, scoring = 'neg_mean_squared_error')\n",
    "gcv_gbt.fit(X_train_std, y_train)\n",
    "print('Best parameter combination for GBT model is\\n', gcv_gbt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c27e9b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The RMSE of the Gradient Boosting Regressor model is 0.1001\n"
     ]
    }
   ],
   "source": [
    "gbt = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 4, max_features = 'auto', min_samples_leaf = 4, min_samples_split = 2, n_estimators = 6, random_state = 0)\n",
    "model = gbt.fit(X_train_std, y_train)\n",
    "\n",
    "# Generating and testing predictions\n",
    "y_val_pred = model.predict(X_val_std)\n",
    "mse_gbt = mean_squared_error(y_val,y_val_pred)\n",
    "print(f'\\nThe RMSE of the Gradient Boosting Regressor model is {np.sqrt(mse_gbt):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbbf05",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127d7a9",
   "metadata": {},
   "source": [
    "After testing multiple prediction models, it was found that the Linear Regression model performed the best. The model and its MSE are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "44d53fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression intercept is 0.7241812528192324\n",
      "\n",
      "Estimated regression parameters:\n",
      "            Variables      Beta\n",
      "0          GRE_Score  0.023699\n",
      "1        TOEFL_Score  0.012982\n",
      "2  University_Rating  0.011583\n",
      "3                SOP -0.000482\n",
      "4                LOR  0.016472\n",
      "5               CGPA  0.062939\n",
      "6           Research  0.014257\n",
      "\n",
      "The MSE of the LR model is 0.0047\n"
     ]
    }
   ],
   "source": [
    "# Building and fitting model to training set (Std data)\n",
    "lr = LinearRegression() \n",
    "model = lr.fit(X_train_std,y_train) \n",
    "\n",
    "print(f'The regression intercept is {model.intercept_}')\n",
    "print(f\"\\nEstimated regression parameters:\\n {pd.DataFrame({'Variables':X_train.columns, 'Beta':model.coef_})}\")\n",
    "\n",
    "# Generating and testing predictions\n",
    "y_val_pred = model.predict(X_val_std)\n",
    "mse_lr = mean_squared_error(y_val,y_val_pred)\n",
    "print(f'\\nThe MSE of the LR model is {mse_lr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1859fe",
   "metadata": {},
   "source": [
    "### Running model on test set to get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "48f34424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping serial number\n",
    "ser_num = test['Serial_No']\n",
    "test = test.drop(test.columns[0], axis = 1)\n",
    "\n",
    "# Standardizing \n",
    "#removing research column\n",
    "test_res = test['Research']\n",
    "test = test.drop('Research', axis = 1)\n",
    "X_test_std = scaler.transform(test)\n",
    "\n",
    "# Dataframe \n",
    "X_test_std = pd.DataFrame(X_test_std, columns = test.columns)\n",
    "#Join 'Research' to training and std_training sets\n",
    "test['Research'] = test_res\n",
    "X_test_std['Research'] = test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "abe15eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_Score</th>\n",
       "      <th>TOEFL_Score</th>\n",
       "      <th>University_Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047606</td>\n",
       "      <td>-0.174868</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-0.647545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.668755</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>0.055486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.283426</td>\n",
       "      <td>-1.381334</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-1.016636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.549632</td>\n",
       "      <td>-1.036630</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-0.823303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.815839</td>\n",
       "      <td>-1.726039</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>-1.702092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.041129</td>\n",
       "      <td>-0.347221</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-0.911182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.846226</td>\n",
       "      <td>0.169836</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>0.231243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.460897</td>\n",
       "      <td>-0.691925</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-1.315425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.307336</td>\n",
       "      <td>-0.519573</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>-1.653977</td>\n",
       "      <td>-1.754819</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.313813</td>\n",
       "      <td>-0.174868</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-2.212093</td>\n",
       "      <td>-0.348757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.402548</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.557841</td>\n",
       "      <td>1.694715</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.467374</td>\n",
       "      <td>1.376302</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>0.557841</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>1.584578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.822316</td>\n",
       "      <td>1.203950</td>\n",
       "      <td>1.597354</td>\n",
       "      <td>1.067335</td>\n",
       "      <td>1.694715</td>\n",
       "      <td>1.039729</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.017219</td>\n",
       "      <td>-0.864277</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>-0.489363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.396071</td>\n",
       "      <td>-1.036630</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>0.459728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.112432</td>\n",
       "      <td>1.031598</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.067335</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>0.934274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.225077</td>\n",
       "      <td>-0.691925</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>1.136600</td>\n",
       "      <td>-0.524515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.573542</td>\n",
       "      <td>-1.726039</td>\n",
       "      <td>-1.904046</td>\n",
       "      <td>-2.499126</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-2.071183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.727103</td>\n",
       "      <td>-2.760152</td>\n",
       "      <td>-1.904046</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-1.653977</td>\n",
       "      <td>-1.333000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.460897</td>\n",
       "      <td>-1.726039</td>\n",
       "      <td>-1.904046</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>-1.069364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.662278</td>\n",
       "      <td>-0.864277</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-0.700273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.580019</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>1.022153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.644845</td>\n",
       "      <td>1.893359</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.067335</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>1.725184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.313813</td>\n",
       "      <td>0.169836</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>-0.172999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.218600</td>\n",
       "      <td>-0.519573</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-1.737243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.194690</td>\n",
       "      <td>-1.208982</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-1.719667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.815839</td>\n",
       "      <td>-2.070743</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>-1.653977</td>\n",
       "      <td>-2.123910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-2.436987</td>\n",
       "      <td>-1.381334</td>\n",
       "      <td>-1.904046</td>\n",
       "      <td>-1.989631</td>\n",
       "      <td>-1.653977</td>\n",
       "      <td>-1.895425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.283426</td>\n",
       "      <td>-1.726039</td>\n",
       "      <td>-1.904046</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-1.737243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.573542</td>\n",
       "      <td>-1.553687</td>\n",
       "      <td>-1.904046</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>-0.366333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.402548</td>\n",
       "      <td>-0.691925</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>0.354274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.136342</td>\n",
       "      <td>-0.519573</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.055486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.580019</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.067335</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>0.899123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1.549632</td>\n",
       "      <td>-1.381334</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-0.665121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.999787</td>\n",
       "      <td>0.859245</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>1.694715</td>\n",
       "      <td>1.936093</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.556109</td>\n",
       "      <td>1.548654</td>\n",
       "      <td>1.597354</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>1.694715</td>\n",
       "      <td>2.076699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.372161</td>\n",
       "      <td>-1.208982</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.970643</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-1.192394</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.928484</td>\n",
       "      <td>-0.519573</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>-0.946333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.904574</td>\n",
       "      <td>-1.898391</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.989631</td>\n",
       "      <td>-1.653977</td>\n",
       "      <td>-1.473607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.225077</td>\n",
       "      <td>-0.002516</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>-0.348757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.307336</td>\n",
       "      <td>-1.036630</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-1.095862</td>\n",
       "      <td>-0.700273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.047606</td>\n",
       "      <td>-0.347221</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>-1.480137</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>0.020334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.757490</td>\n",
       "      <td>0.686893</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>0.557841</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.846395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.041129</td>\n",
       "      <td>-0.691925</td>\n",
       "      <td>-1.028696</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>-0.537746</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.023697</td>\n",
       "      <td>0.514541</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.067335</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>1.039729</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.580019</td>\n",
       "      <td>0.342188</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.705789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.668755</td>\n",
       "      <td>-0.174868</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>-0.461148</td>\n",
       "      <td>0.020369</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.112432</td>\n",
       "      <td>1.376302</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>1.136600</td>\n",
       "      <td>1.426396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.484807</td>\n",
       "      <td>-0.864277</td>\n",
       "      <td>-0.153346</td>\n",
       "      <td>0.048346</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>0.248819</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.378638</td>\n",
       "      <td>1.548654</td>\n",
       "      <td>0.722004</td>\n",
       "      <td>1.576829</td>\n",
       "      <td>0.578485</td>\n",
       "      <td>1.795487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GRE_Score  TOEFL_Score  University_Rating       SOP       LOR      CGPA  \\\n",
       "0    0.047606    -0.174868          -0.153346 -0.461148  0.020369 -0.647545   \n",
       "1    0.668755     0.342188           0.722004  0.048346  0.578485  0.055486   \n",
       "2   -1.283426    -1.381334          -1.028696 -0.461148  0.020369 -1.016636   \n",
       "3   -1.549632    -1.036630          -0.153346  0.048346 -1.095862 -0.823303   \n",
       "4   -1.815839    -1.726039          -1.028696 -0.970643 -0.537746 -1.702092   \n",
       "5   -0.041129    -0.347221          -1.028696 -1.480137  0.020369 -0.911182   \n",
       "6    0.846226     0.169836          -0.153346  0.048346  0.578485  0.231243   \n",
       "7   -1.460897    -0.691925          -1.028696  0.048346  0.020369 -1.315425   \n",
       "8   -0.307336    -0.519573          -1.028696 -0.970643 -1.653977 -1.754819   \n",
       "9    0.313813    -0.174868          -1.028696 -1.480137 -2.212093 -0.348757   \n",
       "10   0.402548     0.342188          -0.153346  0.557841  1.694715  0.002758   \n",
       "11   1.467374     1.376302           0.722004  0.557841  0.020369  1.584578   \n",
       "12   1.822316     1.203950           1.597354  1.067335  1.694715  1.039729   \n",
       "13  -1.017219    -0.864277          -1.028696 -0.970643 -0.537746 -0.489363   \n",
       "14  -0.396071    -1.036630          -0.153346  0.048346  0.578485  0.459728   \n",
       "15   1.112432     1.031598           0.722004  1.067335 -0.537746  0.934274   \n",
       "16   0.225077    -0.691925          -0.153346  0.048346  1.136600 -0.524515   \n",
       "17  -0.573542    -1.726039          -1.904046 -2.499126 -1.095862 -2.071183   \n",
       "18  -1.727103    -2.760152          -1.904046 -1.480137 -1.653977 -1.333000   \n",
       "19  -1.460897    -1.726039          -1.904046 -1.480137 -0.537746 -1.069364   \n",
       "20  -0.662278    -0.864277          -1.028696 -0.970643 -1.095862 -0.700273   \n",
       "21   0.580019     0.342188          -0.153346  0.048346 -0.537746  1.022153   \n",
       "22   1.644845     1.893359           0.722004  1.067335  0.578485  1.725184   \n",
       "23   0.313813     0.169836          -0.153346 -0.461148 -0.537746 -0.172999   \n",
       "24  -0.218600    -0.519573          -1.028696 -1.480137 -1.095862 -1.737243   \n",
       "25  -1.194690    -1.208982          -1.028696 -1.480137 -1.095862 -1.719667   \n",
       "26  -1.815839    -2.070743          -1.028696 -0.970643 -1.653977 -2.123910   \n",
       "27  -2.436987    -1.381334          -1.904046 -1.989631 -1.653977 -1.895425   \n",
       "28  -1.283426    -1.726039          -1.904046 -1.480137 -1.095862 -1.737243   \n",
       "29  -0.573542    -1.553687          -1.904046 -0.970643 -0.537746 -0.366333   \n",
       "30   0.402548    -0.691925          -0.153346  0.048346  0.578485  0.354274   \n",
       "31   0.136342    -0.519573          -0.153346 -0.461148  0.020369  0.055486   \n",
       "32   0.580019     0.342188           0.722004  1.067335  0.578485  0.899123   \n",
       "33  -1.549632    -1.381334          -0.153346 -0.461148  0.020369 -0.665121   \n",
       "34   1.999787     0.859245           0.722004  1.576829  1.694715  1.936093   \n",
       "35   1.556109     1.548654           1.597354  1.576829  1.694715  2.076699   \n",
       "36  -1.372161    -1.208982          -1.028696 -0.970643  0.020369 -1.192394   \n",
       "37  -0.928484    -0.519573          -1.028696 -1.480137  0.020369 -0.946333   \n",
       "38  -1.904574    -1.898391          -1.028696 -1.989631 -1.653977 -1.473607   \n",
       "39   0.225077    -0.002516          -0.153346  0.048346  0.578485 -0.348757   \n",
       "40  -0.307336    -1.036630          -1.028696 -1.480137 -1.095862 -0.700273   \n",
       "41   0.047606    -0.347221          -0.153346 -1.480137 -0.537746  0.020334   \n",
       "42   0.757490     0.686893           0.722004  0.557841  0.020369  0.846395   \n",
       "43  -0.041129    -0.691925          -1.028696 -0.461148 -0.537746  0.213667   \n",
       "44   1.023697     0.514541           0.722004  1.067335  0.578485  1.039729   \n",
       "45   0.580019     0.342188          -0.153346  0.048346  0.020369  0.705789   \n",
       "46   0.668755    -0.174868          -0.153346 -0.461148  0.020369  0.828819   \n",
       "47   1.112432     1.376302           0.722004  1.576829  1.136600  1.426396   \n",
       "48  -0.484807    -0.864277          -0.153346  0.048346  0.578485  0.248819   \n",
       "49   1.378638     1.548654           0.722004  1.576829  0.578485  1.795487   \n",
       "\n",
       "    Research  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          1  \n",
       "7          1  \n",
       "8          0  \n",
       "9          0  \n",
       "10         1  \n",
       "11         1  \n",
       "12         1  \n",
       "13         0  \n",
       "14         1  \n",
       "15         1  \n",
       "16         1  \n",
       "17         0  \n",
       "18         0  \n",
       "19         1  \n",
       "20         0  \n",
       "21         1  \n",
       "22         1  \n",
       "23         1  \n",
       "24         0  \n",
       "25         0  \n",
       "26         0  \n",
       "27         0  \n",
       "28         0  \n",
       "29         1  \n",
       "30         1  \n",
       "31         1  \n",
       "32         1  \n",
       "33         0  \n",
       "34         1  \n",
       "35         1  \n",
       "36         0  \n",
       "37         0  \n",
       "38         0  \n",
       "39         1  \n",
       "40         0  \n",
       "41         0  \n",
       "42         1  \n",
       "43         0  \n",
       "44         1  \n",
       "45         1  \n",
       "46         1  \n",
       "47         1  \n",
       "48         0  \n",
       "49         1  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d211296e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69532259, 0.78009034, 0.61474675, 0.6023315 , 0.53130786,\n",
       "       0.65048358, 0.78298115, 0.6004397 , 0.56101519, 0.65975874,\n",
       "       0.77846389, 0.89924169, 0.90859721, 0.63774893, 0.75225846,\n",
       "       0.8359872 , 0.7187001 , 0.51892307, 0.51493617, 0.58390593,\n",
       "       0.6236933 , 0.81030238, 0.92795691, 0.72678046, 0.57366298,\n",
       "       0.54268708, 0.48189819, 0.48085943, 0.52262606, 0.651175  ,\n",
       "       0.76902265, 0.73719845, 0.83059357, 0.62644411, 0.95435899,\n",
       "       0.97178314, 0.58980758, 0.62500433, 0.52345305, 0.7295187 ,\n",
       "       0.63011323, 0.71216136, 0.82700825, 0.70712094, 0.85219527,\n",
       "       0.79958394, 0.80296354, 0.89876914, 0.72486132, 0.92135242])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_std)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "da55a7b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_Score</th>\n",
       "      <th>TOEFL_Score</th>\n",
       "      <th>University_Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Serial_No</th>\n",
       "      <th>Chance_of_Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>318</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.27</td>\n",
       "      <td>1</td>\n",
       "      <td>351</td>\n",
       "      <td>0.695323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>0.780090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.06</td>\n",
       "      <td>1</td>\n",
       "      <td>353</td>\n",
       "      <td>0.614747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>0.602331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>297</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0</td>\n",
       "      <td>355</td>\n",
       "      <td>0.531308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>317</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0</td>\n",
       "      <td>356</td>\n",
       "      <td>0.650484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>327</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.77</td>\n",
       "      <td>1</td>\n",
       "      <td>357</td>\n",
       "      <td>0.782981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>301</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.89</td>\n",
       "      <td>1</td>\n",
       "      <td>358</td>\n",
       "      <td>0.600440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>314</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0</td>\n",
       "      <td>359</td>\n",
       "      <td>0.561015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>321</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.659759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.64</td>\n",
       "      <td>1</td>\n",
       "      <td>361</td>\n",
       "      <td>0.778464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>334</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.54</td>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>0.899242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>338</td>\n",
       "      <td>115</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "      <td>363</td>\n",
       "      <td>0.908597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>306</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.36</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>0.637749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>313</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.90</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>0.752258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>330</td>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>0.835987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>320</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.34</td>\n",
       "      <td>1</td>\n",
       "      <td>367</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>311</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>0.518923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>298</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>0.514936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>301</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.03</td>\n",
       "      <td>1</td>\n",
       "      <td>370</td>\n",
       "      <td>0.583906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>310</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>0.623693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.22</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>0.810302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>336</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.62</td>\n",
       "      <td>1</td>\n",
       "      <td>373</td>\n",
       "      <td>0.927957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>321</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "      <td>374</td>\n",
       "      <td>0.726780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>315</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>0.573663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>304</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.66</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>0.542687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>297</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>0.481898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>290</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.56</td>\n",
       "      <td>0</td>\n",
       "      <td>378</td>\n",
       "      <td>0.480859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>303</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>0.522626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>311</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>1</td>\n",
       "      <td>380</td>\n",
       "      <td>0.651175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>322</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "      <td>381</td>\n",
       "      <td>0.769023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>319</td>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>382</td>\n",
       "      <td>0.737198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.15</td>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "      <td>0.830594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>0.626444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>340</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.74</td>\n",
       "      <td>1</td>\n",
       "      <td>385</td>\n",
       "      <td>0.954359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>335</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.82</td>\n",
       "      <td>1</td>\n",
       "      <td>386</td>\n",
       "      <td>0.971783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>302</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "      <td>0.589808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>307</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0</td>\n",
       "      <td>388</td>\n",
       "      <td>0.625004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>296</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "      <td>389</td>\n",
       "      <td>0.523453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>320</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.44</td>\n",
       "      <td>1</td>\n",
       "      <td>390</td>\n",
       "      <td>0.729519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>314</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0</td>\n",
       "      <td>391</td>\n",
       "      <td>0.630113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>318</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0</td>\n",
       "      <td>392</td>\n",
       "      <td>0.712161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>326</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1</td>\n",
       "      <td>393</td>\n",
       "      <td>0.827008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>317</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.76</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>0.707121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>329</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "      <td>395</td>\n",
       "      <td>0.852195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>396</td>\n",
       "      <td>0.799584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "      <td>0.802964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>398</td>\n",
       "      <td>0.898769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>0.724861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>0.921352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    GRE_Score  TOEFL_Score  University_Rating  SOP  LOR  CGPA  Research  \\\n",
       "0         318          107                  3  3.0  3.5  8.27         1   \n",
       "1         325          110                  4  3.5  4.0  8.67         1   \n",
       "2         303          100                  2  3.0  3.5  8.06         1   \n",
       "3         300          102                  3  3.5  2.5  8.17         0   \n",
       "4         297           98                  2  2.5  3.0  7.67         0   \n",
       "5         317          106                  2  2.0  3.5  8.12         0   \n",
       "6         327          109                  3  3.5  4.0  8.77         1   \n",
       "7         301          104                  2  3.5  3.5  7.89         1   \n",
       "8         314          105                  2  2.5  2.0  7.64         0   \n",
       "9         321          107                  2  2.0  1.5  8.44         0   \n",
       "10        322          110                  3  4.0  5.0  8.64         1   \n",
       "11        334          116                  4  4.0  3.5  9.54         1   \n",
       "12        338          115                  5  4.5  5.0  9.23         1   \n",
       "13        306          103                  2  2.5  3.0  8.36         0   \n",
       "14        313          102                  3  3.5  4.0  8.90         1   \n",
       "15        330          114                  4  4.5  3.0  9.17         1   \n",
       "16        320          104                  3  3.5  4.5  8.34         1   \n",
       "17        311           98                  1  1.0  2.5  7.46         0   \n",
       "18        298           92                  1  2.0  2.0  7.88         0   \n",
       "19        301           98                  1  2.0  3.0  8.03         1   \n",
       "20        310          103                  2  2.5  2.5  8.24         0   \n",
       "21        324          110                  3  3.5  3.0  9.22         1   \n",
       "22        336          119                  4  4.5  4.0  9.62         1   \n",
       "23        321          109                  3  3.0  3.0  8.54         1   \n",
       "24        315          105                  2  2.0  2.5  7.65         0   \n",
       "25        304          101                  2  2.0  2.5  7.66         0   \n",
       "26        297           96                  2  2.5  2.0  7.43         0   \n",
       "27        290          100                  1  1.5  2.0  7.56         0   \n",
       "28        303           98                  1  2.0  2.5  7.65         0   \n",
       "29        311           99                  1  2.5  3.0  8.43         1   \n",
       "30        322          104                  3  3.5  4.0  8.84         1   \n",
       "31        319          105                  3  3.0  3.5  8.67         1   \n",
       "32        324          110                  4  4.5  4.0  9.15         1   \n",
       "33        300          100                  3  3.0  3.5  8.26         0   \n",
       "34        340          113                  4  5.0  5.0  9.74         1   \n",
       "35        335          117                  5  5.0  5.0  9.82         1   \n",
       "36        302          101                  2  2.5  3.5  7.96         0   \n",
       "37        307          105                  2  2.0  3.5  8.10         0   \n",
       "38        296           97                  2  1.5  2.0  7.80         0   \n",
       "39        320          108                  3  3.5  4.0  8.44         1   \n",
       "40        314          102                  2  2.0  2.5  8.24         0   \n",
       "41        318          106                  3  2.0  3.0  8.65         0   \n",
       "42        326          112                  4  4.0  3.5  9.12         1   \n",
       "43        317          104                  2  3.0  3.0  8.76         0   \n",
       "44        329          111                  4  4.5  4.0  9.23         1   \n",
       "45        324          110                  3  3.5  3.5  9.04         1   \n",
       "46        325          107                  3  3.0  3.5  9.11         1   \n",
       "47        330          116                  4  5.0  4.5  9.45         1   \n",
       "48        312          103                  3  3.5  4.0  8.78         0   \n",
       "49        333          117                  4  5.0  4.0  9.66         1   \n",
       "\n",
       "    Serial_No  Chance_of_Admit  \n",
       "0         351         0.695323  \n",
       "1         352         0.780090  \n",
       "2         353         0.614747  \n",
       "3         354         0.602331  \n",
       "4         355         0.531308  \n",
       "5         356         0.650484  \n",
       "6         357         0.782981  \n",
       "7         358         0.600440  \n",
       "8         359         0.561015  \n",
       "9         360         0.659759  \n",
       "10        361         0.778464  \n",
       "11        362         0.899242  \n",
       "12        363         0.908597  \n",
       "13        364         0.637749  \n",
       "14        365         0.752258  \n",
       "15        366         0.835987  \n",
       "16        367         0.718700  \n",
       "17        368         0.518923  \n",
       "18        369         0.514936  \n",
       "19        370         0.583906  \n",
       "20        371         0.623693  \n",
       "21        372         0.810302  \n",
       "22        373         0.927957  \n",
       "23        374         0.726780  \n",
       "24        375         0.573663  \n",
       "25        376         0.542687  \n",
       "26        377         0.481898  \n",
       "27        378         0.480859  \n",
       "28        379         0.522626  \n",
       "29        380         0.651175  \n",
       "30        381         0.769023  \n",
       "31        382         0.737198  \n",
       "32        383         0.830594  \n",
       "33        384         0.626444  \n",
       "34        385         0.954359  \n",
       "35        386         0.971783  \n",
       "36        387         0.589808  \n",
       "37        388         0.625004  \n",
       "38        389         0.523453  \n",
       "39        390         0.729519  \n",
       "40        391         0.630113  \n",
       "41        392         0.712161  \n",
       "42        393         0.827008  \n",
       "43        394         0.707121  \n",
       "44        395         0.852195  \n",
       "45        396         0.799584  \n",
       "46        397         0.802964  \n",
       "47        398         0.898769  \n",
       "48        399         0.724861  \n",
       "49        400         0.921352  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Serial_No'] = ser_num\n",
    "test['Chance_of_Admit'] = y_test_pred\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bdd5199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('C:\\\\Users\\\\m.maraqa\\\\Desktop\\\\MMA courses\\\\MDN Data Challenge\\\\Graduating_Class.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
